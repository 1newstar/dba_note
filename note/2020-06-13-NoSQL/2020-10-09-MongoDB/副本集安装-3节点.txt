
大纲 
	1. 实验环境
	2. 启动mongodb
	3. 进入3个实例中的任何一个，初始化副本集
	4. 查看副本集状态
	5. 测试副本集
	6. 把主节点变为备份节点之后的各个节点的日志
	7. 测试副本集故障转移功能
	8. 查看复制是否有延迟
	
	
实验环境
	hostname      主机IP          Version  Role
	mongodb01	  192.168.0.121   4.0.13   PRIMARY
	mongodb02	  192.168.0.122   4.0.13   SECONDARY
	mongodb03	  192.168.0.123   4.0.13   SECONDARY

启动mongodb
mongod -f /etc/mongodb.conf

	参考 mongodb.conf 文件

	
进入3个实例中的任何一个，初始化副本集

	mongo --host ip_addr

	mongo --host localhost --port 27017

	
	configs={_id:"repl_set",members:[{_id:0,host:"192.168.0.121:27017"},{_id:1,host:"192.168.0.122:27017"},{_id:2,host:"192.168.0.123:27017"}]};
	{
		"_id" : "repl_set",
		"members" : [
			{
				"_id" : 0,
				"host" : "192.168.0.121:27017"
			},
			{
				"_id" : 1,
				"host" : "192.168.0.122:27017"
			},
			{
				"_id" : 2,
				"host" : "192.168.0.123:27017"
			}
		]
	}

	初始化配置
		> rs.initiate(configs);
		{
			"ok" : 1,
			"operationTime" : Timestamp(1574826701, 1),
			"$clusterTime" : {
				"clusterTime" : Timestamp(1574826701, 1),
				"signature" : {
					"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
					"keyId" : NumberLong(0)
				}
			}
		}
		repl_set:SECONDARY> 
		
		# 对应的日志
			2019-11-27T11:51:40.926+0800 I REPL     [conn4] replSetInitiate admin command received from client
			2019-11-27T11:51:40.931+0800 I REPL     [conn4] replSetInitiate config object with 3 members parses ok
			2019-11-27T11:51:40.931+0800 I ASIO     [Replication] Connecting to 192.168.0.122:27017
			2019-11-27T11:51:40.931+0800 I ASIO     [Replication] Connecting to 192.168.0.123:27017
			2019-11-27T11:51:40.937+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:48114 #9 (2 connections now open)
			2019-11-27T11:51:40.938+0800 I NETWORK  [conn9] received client metadata from 192.168.0.122:48114 conn9: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:40.939+0800 I REPL     [conn4] ******
			2019-11-27T11:51:40.939+0800 I REPL     [conn4] creating replication oplog of size: 1444MB...
			2019-11-27T11:51:40.939+0800 I STORAGE  [conn4] createCollection: local.oplog.rs with generated UUID: e6af4436-bc41-43d5-9506-c098f6d6ea59
			2019-11-27T11:51:40.940+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44882 #10 (3 connections now open)
			2019-11-27T11:51:40.940+0800 I NETWORK  [conn10] received client metadata from 192.168.0.123:44882 conn10: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:40.947+0800 I STORAGE  [conn4] Starting OplogTruncaterThread local.oplog.rs
			2019-11-27T11:51:40.947+0800 I STORAGE  [conn4] The size storer reports that the oplog contains 0 records totaling to 0 bytes
			2019-11-27T11:51:40.947+0800 I STORAGE  [conn4] Scanning the oplog to determine where to place markers for truncation
			2019-11-27T11:51:41.078+0800 I REPL     [conn4] ******
			2019-11-27T11:51:41.078+0800 I STORAGE  [conn4] createCollection: local.system.replset with generated UUID: addc43cd-78f7-4ae7-8d92-f76dc1d710e5
			2019-11-27T11:51:41.095+0800 I STORAGE  [conn4] createCollection: admin.system.version with provided UUID: ff2be7ec-7113-4a5d-aa80-0ae7c2c2d461
			2019-11-27T11:51:41.156+0800 I COMMAND  [conn4] setting featureCompatibilityVersion to 4.0
			2019-11-27T11:51:41.156+0800 I NETWORK  [conn4] Skip closing connection for connection # 10
			2019-11-27T11:51:41.156+0800 I NETWORK  [conn4] Skip closing connection for connection # 9
			2019-11-27T11:51:41.156+0800 I NETWORK  [conn4] Skip closing connection for connection # 4
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] New replica set config in use: { _id: "repl_set", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "192.168.0.121:27017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "192.168.0.122:27017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "192.168.0.123:27017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5dddf2cc06e25c57fda787eb') } }
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] This node is 192.168.0.121:27017 in the config
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] transition to STARTUP2 from STARTUP
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] Starting replication storage threads
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] transition to RECOVERING from STARTUP2
			2019-11-27T11:51:41.156+0800 I REPL     [conn4] Starting replication fetcher thread
			2019-11-27T11:51:41.157+0800 I REPL     [conn4] Starting replication applier thread
			2019-11-27T11:51:41.157+0800 I REPL     [conn4] Starting replication reporter thread
			2019-11-27T11:51:41.157+0800 I COMMAND  [conn4] command local.system.replset appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: { _id: "repl_set", members: [ { _id: 0.0, host: "192.168.0.121:27017" }, { _id: 1.0, host: "192.168.0.122:27017" }, { _id: 2.0, host: "192.168.0.123:27017" } ] }, lsid: { id: UUID("57617b96-8d0f-4af5-adcf-7f0c57951ce8") }, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "admin" } numYields:0 reslen:163 locks:{ Global: { acquireCount: { r: 14, w: 6, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 432 } }, Database: { acquireCount: { r: 2, w: 3, W: 3 } }, Collection: { acquireCount: { r: 1, w: 2 } }, oplog: { acquireCount: { r: 1, w: 2 } } } storage:{} protocol:op_msg 230ms
			2019-11-27T11:51:41.158+0800 I REPL     [replexec-0] Member 192.168.0.122:27017 is now in state STARTUP
			2019-11-27T11:51:41.158+0800 I REPL     [replexec-0] Member 192.168.0.123:27017 is now in state STARTUP
			2019-11-27T11:51:41.158+0800 I REPL     [rsSync-0] Starting oplog application
			2019-11-27T11:51:41.158+0800 I REPL     [rsSync-0] transition to SECONDARY from RECOVERING
			2019-11-27T11:51:41.158+0800 I REPL     [rsSync-0] Resetting sync source to empty, which was :27017
			2019-11-27T11:51:42.940+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:48116 #11 (4 connections now open)
			2019-11-27T11:51:42.941+0800 I NETWORK  [conn11] end connection 192.168.0.122:48116 (3 connections now open)
			2019-11-27T11:51:42.943+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44884 #12 (4 connections now open)
			2019-11-27T11:51:42.944+0800 I NETWORK  [conn12] end connection 192.168.0.123:44884 (3 connections now open)
			2019-11-27T11:51:43.059+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:48122 #13 (4 connections now open)
			2019-11-27T11:51:43.059+0800 I NETWORK  [conn13] received client metadata from 192.168.0.122:48122 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:43.061+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:48124 #14 (5 connections now open)
			2019-11-27T11:51:43.061+0800 I NETWORK  [conn14] received client metadata from 192.168.0.122:48124 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:43.100+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44890 #15 (6 connections now open)
			2019-11-27T11:51:43.100+0800 I NETWORK  [conn15] received client metadata from 192.168.0.123:44890 conn15: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:43.102+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44892 #16 (7 connections now open)
			2019-11-27T11:51:43.102+0800 I NETWORK  [conn16] received client metadata from 192.168.0.123:44892 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:43.161+0800 I REPL     [replexec-1] Member 192.168.0.122:27017 is now in state SECONDARY
			2019-11-27T11:51:43.161+0800 I REPL     [replexec-1] Member 192.168.0.123:27017 is now in state SECONDARY
			2019-11-27T11:51:52.312+0800 I REPL     [replexec-0] Starting an election, since we've seen no PRIMARY in the past 10000ms
			2019-11-27T11:51:52.312+0800 I REPL     [replexec-0] conducting a dry run election to see if we could be elected. current term: 0
			2019-11-27T11:51:52.313+0800 I REPL     [replexec-1] VoteRequester(term 0 dry run) received a yes vote from 192.168.0.122:27017; response message: { term: 0, voteGranted: true, reason: "", ok: 1.0, operationTime: Timestamp(1574826701, 1), $clusterTime: { clusterTime: Timestamp(1574826701, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
			2019-11-27T11:51:52.313+0800 I REPL     [replexec-1] dry election run succeeded, running for election in term 1
			2019-11-27T11:51:52.317+0800 I REPL     [replexec-0] VoteRequester(term 1) received a yes vote from 192.168.0.123:27017; response message: { term: 1, voteGranted: true, reason: "", ok: 1.0, operationTime: Timestamp(1574826701, 1), $clusterTime: { clusterTime: Timestamp(1574826701, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
			2019-11-27T11:51:52.317+0800 I REPL     [replexec-0] election succeeded, assuming primary role in term 1
			2019-11-27T11:51:52.317+0800 I REPL     [replexec-0] transition to PRIMARY from SECONDARY
			2019-11-27T11:51:52.317+0800 I REPL     [replexec-0] Resetting sync source to empty, which was :27017
			2019-11-27T11:51:52.317+0800 I REPL     [replexec-0] Entering primary catch-up mode.
			2019-11-27T11:51:52.318+0800 I REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1574826701, 1), t: -1 }. My Last Applied: { ts: Timestamp(1574826701, 1), t: -1 }
			2019-11-27T11:51:52.318+0800 I REPL     [replexec-0] Exited primary catch-up mode.
			2019-11-27T11:51:52.318+0800 I REPL     [replexec-0] Stopping replication producer
			2019-11-27T11:51:53.061+0800 I NETWORK  [conn14] end connection 192.168.0.122:48124 (6 connections now open)
			2019-11-27T11:51:53.102+0800 I NETWORK  [conn16] end connection 192.168.0.123:44892 (5 connections now open)
			2019-11-27T11:51:54.163+0800 I STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: d777fe0f-c915-4997-a613-a0962418c258
			2019-11-27T11:51:54.179+0800 I REPL     [rsSync-0] transition to primary complete; database writes are now permitted
			2019-11-27T11:51:54.179+0800 I STORAGE  [monitoring keys for HMAC] createCollection: admin.system.keys with generated UUID: 16bb9719-2754-4901-8011-301baf3b767b
			2019-11-27T11:51:55.101+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:48126 #17 (6 connections now open)
			2019-11-27T11:51:55.102+0800 I NETWORK  [conn17] received client metadata from 192.168.0.122:48126 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:55.109+0800 I STORAGE  [conn13] Triggering the first stable checkpoint. Initial Data: Timestamp(1574826701, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1574826714, 1)
			2019-11-27T11:51:55.148+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44894 #18 (7 connections now open)
			2019-11-27T11:51:55.149+0800 I NETWORK  [conn18] received client metadata from 192.168.0.123:44894 conn18: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
			2019-11-27T11:51:55.244+0800 I COMMAND  [monitoring keys for HMAC] command admin.system.keys command: insert { insert: "system.keys", bypassDocumentValidation: false, ordered: true, documents: [ { _id: 6763829233497145346, purpose: "HMAC", key: BinData(0, 64A30081AB31F3631CBFF0A75CFCF644F86F2618), expiresAt: Timestamp(1582602714, 0) } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, $db: "admin" } ninserted:1 keysInserted:1 numYields:0 reslen:230 locks:{ Global: { acquireCount: { r: 3, w: 3 } }, Database: { acquireCount: { W: 3 } }, Collection: { acquireCount: { w: 2 } } } storage:{} protocol:op_msg 1064ms


查看副本集状态
	repl_set:PRIMARY> 
	repl_set:PRIMARY> rs.status();
	{
		"set" : "repl_set",
		"date" : ISODate("2019-11-27T04:22:46.657Z"),
		"myState" : 1,
		"term" : NumberLong(1),
		"syncingTo" : "",
		"syncSourceHost" : "",
		"syncSourceId" : -1,
		"heartbeatIntervalMillis" : NumberLong(2000),
		"optimes" : {
			"lastCommittedOpTime" : {
				"ts" : Timestamp(1574828564, 1),
				"t" : NumberLong(1)
			},
			"readConcernMajorityOpTime" : {
				"ts" : Timestamp(1574828564, 1),
				"t" : NumberLong(1)
			},
			"appliedOpTime" : {
				"ts" : Timestamp(1574828564, 1),
				"t" : NumberLong(1)
			},
			"durableOpTime" : {
				"ts" : Timestamp(1574828564, 1),
				"t" : NumberLong(1)
			}
		},
		"lastStableCheckpointTimestamp" : Timestamp(1574828514, 1),
		"members" : [
			{
				"_id" : 0,
				"name" : "192.168.0.121:27017",
				"health" : 1,
				"state" : 1,
				"stateStr" : "PRIMARY",
				"uptime" : 2493,
				"optime" : {
					"ts" : Timestamp(1574828564, 1),
					"t" : NumberLong(1)
				},
				"optimeDate" : ISODate("2019-11-27T04:22:44Z"),
				"syncingTo" : "",
				"syncSourceHost" : "",
				"syncSourceId" : -1,
				"infoMessage" : "",
				"electionTime" : Timestamp(1574826712, 1),
				"electionDate" : ISODate("2019-11-27T03:51:52Z"),
				"configVersion" : 1,
				"self" : true,
				"lastHeartbeatMessage" : ""
			},
			{
				"_id" : 1,
				"name" : "192.168.0.122:27017",
				"health" : 1,
				"state" : 2,
				"stateStr" : "SECONDARY",
				"uptime" : 1865,
				"optime" : {
					"ts" : Timestamp(1574828564, 1),
					"t" : NumberLong(1)
				},
				"optimeDurable" : {
					"ts" : Timestamp(1574828564, 1),
					"t" : NumberLong(1)
				},
				"optimeDate" : ISODate("2019-11-27T04:22:44Z"),
				"optimeDurableDate" : ISODate("2019-11-27T04:22:44Z"),
				"lastHeartbeat" : ISODate("2019-11-27T04:22:44.982Z"),
				"lastHeartbeatRecv" : ISODate("2019-11-27T04:22:45.689Z"),
				"pingMs" : NumberLong(0),
				"lastHeartbeatMessage" : "",
				"syncingTo" : "192.168.0.121:27017",
				"syncSourceHost" : "192.168.0.121:27017",
				"syncSourceId" : 0,
				"infoMessage" : "",
				"configVersion" : 1
			},
			{
				"_id" : 2,
				"name" : "192.168.0.123:27017",
				"health" : 1,
				"state" : 2,
				"stateStr" : "SECONDARY",
				"uptime" : 1865,
				"optime" : {
					"ts" : Timestamp(1574828564, 1),
					"t" : NumberLong(1)
				},
				"optimeDurable" : {
					"ts" : Timestamp(1574828564, 1),
					"t" : NumberLong(1)
				},
				"optimeDate" : ISODate("2019-11-27T04:22:44Z"),
				"optimeDurableDate" : ISODate("2019-11-27T04:22:44Z"),
				"lastHeartbeat" : ISODate("2019-11-27T04:22:44.978Z"),
				"lastHeartbeatRecv" : ISODate("2019-11-27T04:22:45.823Z"),
				"pingMs" : NumberLong(0),
				"lastHeartbeatMessage" : "",
				"syncingTo" : "192.168.0.121:27017",
				"syncSourceHost" : "192.168.0.121:27017",
				"syncSourceId" : 0,
				"infoMessage" : "",
				"configVersion" : 1
			}
		],
		"ok" : 1,
		"operationTime" : Timestamp(1574828564, 1),
		"$clusterTime" : {
			"clusterTime" : Timestamp(1574828564, 1),
			"signature" : {
				"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
				"keyId" : NumberLong(0)
			}
		}
	}

	
测试副本集

	主库写入数据:
		repl_set:PRIMARY> use test

		switched to db test

			db.persons.insertMany([{"name":"tom", "age": 20}, {"name":"jack", "age":21}, {"name":"luna", "age":18}])
			db.tests.insert({"name":"菜鸟教程"})

			configsvr = true :

				"errmsg" : "can't create user databases on a --configsvr instance"
	
		repl_set:PRIMARY> repl_set:PRIMARY> use test_db
		switched to db test_db
		repl_set:PRIMARY> db.tests.insert({"name":"菜鸟教程"})
		WriteResult({ "nInserted" : 1 })
		repl_set:PRIMARY> db.tests.find();
		{ "_id" : ObjectId("5dddf99488939aa6ab659919"), "name" : "菜鸟教程" }

	从库读取数据:  # 在 mongodb02上查询
		
		mongo --host 192.168.0.122 --port 27017
		repl_set:SECONDARY> use test_db
		switched to db admin

		repl_set:SECONDARY> db.tests.find();
		{ "_id" : ObjectId("5dddf99488939aa6ab659919"), "name" : "菜鸟教程" }
				
	

		
把主节点变为备份节点之后的各个节点的日志
	
	# replSetStepDown 命令将复制集主节点降级为从节点
	
	repl_set:PRIMARY> rs.stepDown()
	2019-11-27T12:23:55.334+0800 I NETWORK  [js] DBClientConnection failed to receive message from localhost:27017 - HostUnreachable: Connection closed by peer
	2019-11-27T12:23:55.334+0800 E QUERY    [js] Error: error doing query: failed: network error while attempting to run command 'replSetStepDown' on host 'localhost:27017'  :
	DB.prototype.runCommand@src/mongo/shell/db.js:168:1
	DB.prototype.adminCommand@src/mongo/shell/db.js:186:16
	rs.stepDown@src/mongo/shell/utils.js:1489:12
	@(shell):1:1
	2019-11-27T12:23:55.336+0800 I NETWORK  [js] trying reconnect to localhost:27017 failed
	2019-11-27T12:23:55.337+0800 I NETWORK  [js] reconnect localhost:27017 ok


		
	[root@mongodb01 log]# tail -f mongod.log 
	
	2019-11-27T12:23:55.332+0800 I COMMAND  [conn4] Attempting to step down in response to replSetStepDown command
	2019-11-27T12:23:55.332+0800 I REPL     [conn4] transition to SECONDARY from PRIMARY
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 18
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 17
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 15
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 13
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 10
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Skip closing connection for connection # 9
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn26] end connection 192.168.0.122:48138 (12 connections now open)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn25] end connection 192.168.0.122:48136 (11 connections now open)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn24] end connection 192.168.0.123:44906 (10 connections now open)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn23] end connection 192.168.0.123:44904 (9 connections now open)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn21] end connection 192.168.0.122:48130 (8 connections now open)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn19] end connection 192.168.0.123:44900 (7 connections now open)
	2019-11-27T12:23:55.333+0800 I REPL     [conn4] Handing off election to 192.168.0.122:27017
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] Error sending response to client: SocketException: Broken pipe. Ending connection from 127.0.0.1:35232 (connection id: 4)
	2019-11-27T12:23:55.333+0800 I NETWORK  [conn4] end connection 127.0.0.1:35232 (6 connections now open)
	2019-11-27T12:23:55.337+0800 I NETWORK  [listener] connection accepted from 127.0.0.1:35242 #27 (7 connections now open)
	2019-11-27T12:23:55.337+0800 I NETWORK  [conn27] received client metadata from 127.0.0.1:35242 conn27: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
	2019-11-27T12:23:57.004+0800 I REPL     [replexec-31] Member 192.168.0.122:27017 is now in state PRIMARY
	2019-11-27T12:23:57.548+0800 I REPL     [rsBackgroundSync] sync source candidate: 192.168.0.122:27017
	2019-11-27T12:23:57.549+0800 I ASIO     [RS] Connecting to 192.168.0.122:27017
	2019-11-27T12:23:57.552+0800 I REPL     [rsBackgroundSync] Changed sync source from empty to 192.168.0.122:27017
	2019-11-27T12:23:57.553+0800 I ASIO     [RS] Connecting to 192.168.0.122:27017
	2019-11-27T12:24:04.247+0800 I NETWORK  [conn17] end connection 192.168.0.122:48126 (6 connections now open)
	2019-11-27T12:24:26.055+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:44908 #30 (7 connections now open)


	[root@mongodb02 log]# tail -f mongod.log 
	
	2019-11-27T12:24:44.389+0800 I COMMAND  [conn3] Received replSetStepUp request
	2019-11-27T12:24:44.389+0800 I REPL     [conn3] Starting an election due to step up request
	2019-11-27T12:24:44.389+0800 I REPL     [conn3] skipping dry run and running for election in term 2
	2019-11-27T12:24:44.393+0800 I REPL     [replexec-1] VoteRequester(term 2) received a yes vote from 192.168.0.121:27017; response message: { term: 2, voteGranted: true, reason: "", ok: 1.0, operationTime: Timestamp(1574828634, 1), $clusterTime: { clusterTime: Timestamp(1574828634, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
	2019-11-27T12:24:44.394+0800 I REPL     [replexec-1] election succeeded, assuming primary role in term 2
	2019-11-27T12:24:44.394+0800 I REPL     [replexec-1] transition to PRIMARY from SECONDARY
	2019-11-27T12:24:44.394+0800 I REPL     [replexec-1] Resetting sync source to empty, which was 192.168.0.121:27017
	2019-11-27T12:24:44.394+0800 I REPL     [replexec-1] Entering primary catch-up mode.
	2019-11-27T12:24:44.394+0800 I ASIO     [Replication] Connecting to 192.168.0.123:27017
	2019-11-27T12:24:44.395+0800 I REPL     [replexec-1] Member 192.168.0.121:27017 is now in state SECONDARY
	2019-11-27T12:24:44.396+0800 I REPL     [replexec-6] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1574828634, 1), t: 1 }. My Last Applied: { ts: Timestamp(1574828634, 1), t: 1 }
	2019-11-27T12:24:44.396+0800 I REPL     [replexec-6] Exited primary catch-up mode.
	2019-11-27T12:24:44.396+0800 I REPL     [replexec-6] Stopping replication producer
	2019-11-27T12:24:44.396+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
	2019-11-27T12:24:46.314+0800 I REPL     [rsSync-0] transition to primary complete; database writes are now permitted
	2019-11-27T12:24:46.605+0800 I NETWORK  [listener] connection accepted from 192.168.0.121:33056 #22 (6 connections now open)
	2019-11-27T12:24:46.605+0800 I NETWORK  [conn22] received client metadata from 192.168.0.121:33056 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
	2019-11-27T12:24:46.609+0800 I NETWORK  [listener] connection accepted from 192.168.0.121:33058 #23 (7 connections now open)
	2019-11-27T12:24:46.610+0800 I NETWORK  [conn23] received client metadata from 192.168.0.121:33058 conn23: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
	2019-11-27T12:24:48.301+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 192.168.0.121:27017: InvalidSyncSource: Sync source was cleared. Was 192.168.0.121:27017
	2019-11-27T12:24:53.302+0800 I CONNPOOL [RS] Ending connection to host 192.168.0.121:27017 due to bad connection status; 1 connections to that host remain open
	2019-11-27T12:24:54.391+0800 I CONNPOOL [Replication] Ending connection to host 192.168.0.123:27017 due to bad connection status; 1 connections to that host remain open
	2019-11-27T12:25:15.112+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:40096 #24 (8 connections now open)
	2019-11-27T12:25:15.112+0800 I NETWORK  [listener] connection accepted from 192.168.0.123:40098 #25 (9 connections now open)
	2019-11-27T12:25:15.112+0800 I NETWORK  [conn25] received client metadata from 192.168.0.123:40098 conn25: { driver: { name: "MongoDB Internal Client", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
	2019-11-27T12:25:15.113+0800 I NETWORK  [conn24] received client metadata from 192.168.0.123:40096 conn24: { driver: { name: "MongoDB Internal Client", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }

	[root@mongodb03 log]# tail -f mongod.log 		
	2019-11-27T12:25:05.609+0800 I NETWORK  [listener] connection accepted from 192.168.0.122:51798 #21 (6 connections now open)
	2019-11-27T12:25:05.610+0800 I NETWORK  [conn21] received client metadata from 192.168.0.122:51798 conn21: { driver: { name: "NetworkInterfaceTL", version: "4.0.13" }, os: { type: "Linux", name: "CentOS Linux release 7.4.1708 (Core) ", architecture: "x86_64", version: "Kernel 3.10.0-693.el7.x86_64" } }
	2019-11-27T12:25:06.116+0800 I REPL     [replexec-10] Member 192.168.0.121:27017 is now in state SECONDARY
	2019-11-27T12:25:06.128+0800 I REPL     [replexec-10] Member 192.168.0.122:27017 is now in state PRIMARY
	2019-11-27T12:25:07.826+0800 I REPL     [replication-0] Restarting oplog query due to error: InterruptedDueToReplStateChange: error in fetcher batch callback :: caused by :: operation was interrupted. Last fetched optime (with hash): { ts: Timestamp(1574828634, 1), t: 1 }[7334473347684533829]. Restarts remaining: 1
	2019-11-27T12:25:07.826+0800 I REPL     [replication-0] Scheduled new oplog query Fetcher source: 192.168.0.121:27017 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1574828634, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(0, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 3230 -- target:192.168.0.121:27017 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1574828634, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(0, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: RetryPolicyImpl maxAttempts: 1 maxTimeMillis: -1ms
	2019-11-27T12:25:15.605+0800 I NETWORK  [conn8] end connection 192.168.0.122:51778 (5 connections now open)
	2019-11-27T12:25:36.324+0800 I NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for repl_set/192.168.0.121:27017,192.168.0.122:27017,192.168.0.123:27017
	2019-11-27T12:25:36.326+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor] Successfully connected to 192.168.0.121:27017 (1 connections now open to 192.168.0.121:27017 with a 5 second timeout)
	2019-11-27T12:25:36.328+0800 I NETWORK  [LogicalSessionCacheRefresh] Successfully connected to 192.168.0.122:27017 (1 connections now open to 192.168.0.122:27017 with a 0 second timeout)
	2019-11-27T12:25:36.329+0800 I NETWORK  [LogicalSessionCacheReap] Successfully connected to 192.168.0.122:27017 (2 connections now open to 192.168.0.122:27017 with a 0 second timeout)


		
	可以看到, mongodb02 已经被选举为 主库.
	在 mongodb02 做数据测试:
	
		repl_set:PRIMARY> use test_db
		switched to db test_db
		repl_set:PRIMARY> db.tests.insert({"name":"菜鸟教程02"})
		WriteResult({ "nInserted" : 1 })
		repl_set:PRIMARY> db.tests.find();
		{ "_id" : ObjectId("5dddf99488939aa6ab659919"), "name" : "菜鸟教程" }
		{ "_id" : ObjectId("5dddfc136170161e8b5767ae"), "name" : "菜鸟教程02" }

		
	在 mongodb01 查询数据:	
		
		
		# 成为新的从节点后, 需要执行 rs.slaveOk() 语句才能做读取操作:
		repl_set:SECONDARY> rs.slaveOk();
		repl_set:SECONDARY> db.tests.find()
		{ "_id" : ObjectId("5dddf99488939aa6ab659919"), "name" : "菜鸟教程" }
		{ "_id" : ObjectId("5dddfc136170161e8b5767ae"), "name" : "菜鸟教程02" }

		
	在 mongodb03 查询数据:	
		repl_set:SECONDARY> rs.slaveOk();
		repl_set:SECONDARY> db.tests.find()
		{ "_id" : ObjectId("5dddf99488939aa6ab659919"), "name" : "菜鸟教程" }
		{ "_id" : ObjectId("5dddfc136170161e8b5767ae"), "name" : "菜鸟教程02" }


测试副本集故障转移功能
	# 在 mongodb02 上执行
	repl_set:PRIMARY> use admin
	switched to db admin
	
	# 关闭MongoDB
		repl_set:PRIMARY> db.shutdownServer()
		2019-11-27T12:33:38.020+0800 I NETWORK  [js] DBClientConnection failed to receive message from localhost:27017 - HostUnreachable: Connection closed by peer
		server should be down...
		2019-11-27T12:33:38.023+0800 I NETWORK  [js] trying reconnect to localhost:27017 failed
		2019-11-27T12:33:38.640+0800 I NETWORK  [js] DBClientConnection failed to receive message from localhost:27017 - HostUnreachable: Connection reset by peer
		2019-11-27T12:33:38.640+0800 I NETWORK  [js] reconnect localhost:27017 failed failed 
		2019-11-27T12:33:38.643+0800 I NETWORK  [js] trying reconnect to localhost:27017 failed
		2019-11-27T12:33:38.643+0800 I NETWORK  [js] reconnect localhost:27017 failed failed 


	# 查看新主的日志
	[root@mongodb01 log]# tail -f mongod.log
		
		收到replSetStepUp请求
		2019-11-27T12:32:48.966+0800 I COMMAND  [conn9] Received replSetStepUp request
		
		进一步要求开始选举
			拥有最新optime（最近一条oplog的时间戳）的节点才能被选为主。
		2019-11-27T12:32:48.966+0800 I REPL     [conn9] Starting an election due to step up request
		2019-11-27T12:32:48.966+0800 I REPL     [conn9] skipping dry run and running for election in term 3
		
		得到了 mongodb03 这个从节点的赞成票 
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] VoteRequester(term 3) received a yes vote from 192.168.0.123:27017; 
			response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, operationTime: Timestamp(1574829216, 1), 
			$clusterTime: { clusterTime: Timestamp(1574829216, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
		
		选举成功
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] election succeeded, assuming primary role in term 3
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] transition to PRIMARY from SECONDARY
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] Resetting sync source to empty, which was 192.168.0.122:27017
		
		进入主追赶模式
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] Entering primary catch-up mode.
		
		mongodb02 处于SECONDARY状态
		2019-11-27T12:32:48.970+0800 I REPL     [replexec-36] Member 192.168.0.122:27017 is now in state SECONDARY
		
		成为PRIMARY后，掌握了心跳了解的最新 oplog 运行时间
			
		2019-11-27T12:32:48.971+0800 I REPL     [replexec-31] Caught up to the latest optime known via heartbeats after becoming primary. 
			Target optime: { ts: Timestamp(1574829216, 1), t: 2 }. My Last Applied: { ts: Timestamp(1574829216, 1), t: 2 }
			
		退出追赶PRIMARY模式
		2019-11-27T12:32:48.971+0800 I REPL     [replexec-31] Exited primary catch-up mode.
		2019-11-27T12:32:48.971+0800 I REPL     [replexec-31] Stopping replication producer
		
		从同步源返回一批数据后，复制生成器停止。
		放弃这批 oplog 条目，重新评估同步源
		2019-11-27T12:32:48.971+0800 I REPL     [rsBackgroundSync] 
			Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  
			Abandoning this batch of oplog entries and re-evaluating our sync source.
			
		2019-11-27T12:32:49.466+0800 I NETWORK  [conn9] end connection 192.168.0.122:48114 (6 connections now open)
		
		完成升级为PRIMARY节点,现在允许写数据库
		2019-11-27T12:32:50.281+0800 I REPL     [rsSync-0] transition to primary complete; database writes are now permitted
		
		从副本集中移除故障的PRIMARY节点
		2019-11-27T12:32:50.970+0800 I CONNPOOL [Replication] dropping unhealthy pooled connection to 192.168.0.122:27017
		2019-11-27T12:32:50.970+0800 I CONNPOOL [Replication] after drop, pool was empty, going to spawn some connections
		2019-11-27T12:32:50.970+0800 I ASIO     [Replication] Connecting to 192.168.0.122:27017
		2019-11-27T12:32:50.971+0800 I ASIO     [Replication] Failed to connect to 192.168.0.122:27017 - HostUnreachable: 
			Error connecting to 192.168.0.122:27017 :: caused by :: Connection refused
		2019-11-27T12:32:50.971+0800 I CONNPOOL [Replication] Dropping all pooled connections to 192.168.0.122:27017 due to HostUnreachable: 
			Error connecting to 192.168.0.122:27017 :: caused by :: Connection refused
		2019-11-27T12:32:50.971+0800 I REPL_HB  [replexec-31] Error in heartbeat (requestId: 2979) to 192.168.0.122:27017, response status: 
			HostUnreachable: Error connecting to 192.168.0.122:27017 :: caused by :: Connection refused
		2019-11-27T12:32:50.972+0800 I ASIO     [Replication] Connecting to 192.168.0.122:27017
		
		小结: 故障转移期间, 直到选举出新主才可以做写入操作, 说明在选举过程中, 如果有写入操作, 那么这部分的写入操作会丢失.
		
		
		
	# 查看故障主节点的日志
		2019-11-27T12:33:38.019+0800 I REPL     [conn29] transition to SECONDARY from PRIMARY
		2019-11-27T12:33:38.019+0800 I NETWORK  [conn29] Skip closing connection for connection # 23
		2019-11-27T12:33:38.019+0800 I NETWORK  [conn29] Skip closing connection for connection # 22
		2019-11-27T12:33:38.019+0800 I NETWORK  [conn29] Skip closing connection for connection # 9
		2019-11-27T12:33:38.019+0800 I NETWORK  [conn29] Skip closing connection for connection # 3
		2019-11-27T12:33:38.019+0800 I NETWORK  [conn28] end connection 192.168.0.121:33068 (11 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn27] end connection 192.168.0.121:33066 (10 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn26] end connection 192.168.0.121:33060 (9 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn25] end connection 192.168.0.123:40098 (8 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn24] end connection 192.168.0.123:40096 (7 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn17] end connection 192.168.0.122:44742 (6 connections now open)
		2019-11-27T12:33:38.020+0800 I NETWORK  [conn13] end connection 192.168.0.123:40084 (5 connections now open)
		2019-11-27T12:33:38.021+0800 I REPL     [conn29] Handing off election to 192.168.0.121:27017
		2019-11-27T12:33:38.021+0800 I COMMAND  [conn29] terminating, shutdown command received 
		{ shutdown: 1.0, lsid: { id: UUID("b5d67d7d-4b28-46d5-9eaa-7045b61dc356") }, $clusterTime: { clusterTime: Timestamp(1574829206, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "admin" }
		2019-11-27T12:33:38.021+0800 I NETWORK  [conn29] shutdown: going to close listening sockets...
		2019-11-27T12:33:38.021+0800 I NETWORK  [conn29] removing socket file: /tmp/mongodb-27017.sock
		2019-11-27T12:33:38.021+0800 I REPL     [conn29] shutting down replication subsystems
		2019-11-27T12:33:38.021+0800 I REPL     [conn29] Stopping replication reporter thread
		2019-11-27T12:33:38.022+0800 I REPL     [conn29] Stopping replication fetcher thread
		2019-11-27T12:33:38.022+0800 I REPL     [conn29] Stopping replication applier thread
		2019-11-27T12:33:38.022+0800 I REPL     [rsSync-0] Finished oplog application
		2019-11-27T12:33:38.518+0800 I REPL     [rsBackgroundSync] Stopping replication producer
		2019-11-27T12:33:38.519+0800 I REPL     [conn29] Stopping replication storage threads
		2019-11-27T12:33:38.519+0800 I ASIO     [RS] Killing all outstanding egress activity.
		2019-11-27T12:33:38.520+0800 I ASIO     [RS] Killing all outstanding egress activity.
		2019-11-27T12:33:38.521+0800 I ASIO     [Replication] Killing all outstanding egress activity.
		2019-11-27T12:33:38.521+0800 I CONNPOOL [Replication] Dropping all pooled connections to 192.168.0.123:27017 due to ShutdownInProgress: Shutting down the connection pool
		2019-11-27T12:33:38.521+0800 I CONNPOOL [Replication] Dropping all pooled connections to 192.168.0.121:27017 due to ShutdownInProgress: Shutting down the connection pool
		2019-11-27T12:33:38.522+0800 I ASIO     [ReplicaSetMonitor-TaskExecutor] Killing all outstanding egress activity.
		2019-11-27T12:33:38.523+0800 I CONTROL  [conn29] Shutting down free monitoring
		2019-11-27T12:33:38.523+0800 I FTDC     [conn29] Shutting down full-time diagnostic data capture
		2019-11-27T12:33:38.533+0800 I STORAGE  [WTOplogJournalThread] oplog journal thread loop shutting down
		2019-11-27T12:33:38.533+0800 I STORAGE  [conn29] WiredTigerKVEngine shutting down
		2019-11-27T12:33:38.534+0800 I STORAGE  [conn29] Shutting down session sweeper thread
		2019-11-27T12:33:38.535+0800 I STORAGE  [conn29] Finished shutting down session sweeper thread
		2019-11-27T12:33:38.637+0800 I STORAGE  [conn29] shutdown: removing fs lock...
		2019-11-27T12:33:38.637+0800 I CONTROL  [conn29] now exiting
		2019-11-27T12:33:38.637+0800 I CONTROL  [conn29] shutting down with code:0



查看复制是否有延迟

	db.test.drop()
	db.test.find()
	
	repl_set:PRIMARY> use test_db
	switched to db test_db
	
	在主节点制造数据
	repl_set:PRIMARY> for(var i=0;i<1000000;i++){db.test.insert({"name":"test"+i,"age":123})}
	
		# 通过执行 db.printReplicationInfo 命令可查看到操作日志的一些基本信息，如日志大小、日志启用时间等。
		repl_set:PRIMARY> db.printReplicationInfo()
		configured oplog size:   1444.5001945495605MB
		log length start to end: 5050secs (1.4hrs)      # 
		oplog first event time:  Wed Nov 27 2019 11:51:41 GMT+0800 (CST)
		oplog last event time:   Wed Nov 27 2019 13:15:51 GMT+0800 (CST)
		now:                     Wed Nov 27 2019 13:15:51 GMT+0800 (CST)

		repl_set:PRIMARY> db.printReplicationInfo()
		configured oplog size:   1444.5001945495605MB
		log length start to end: 5570secs (1.55hrs)
		oplog first event time:  Wed Nov 27 2019 11:51:41 GMT+0800 (CST)
		oplog last event time:   Wed Nov 27 2019 13:24:31 GMT+0800 (CST)
		now:                     Wed Nov 27 2019 13:24:31 GMT+0800 (CST)

				
		字段说明

			configured oplog size : 配置的oplog文件大小。
			log length start to end : oplog日志的启用时间段。
			oplog first event time : 第一个事务日志的产生时间。
			oplog last event time : 最后一个事务日志的产生时间。
			now : 现在的时间。
	
		
		repl_set:PRIMARY> db.printSlaveReplicationInfo()
		source: 192.168.0.121:27017
			syncedTo: Wed Nov 27 2019 13:16:45 GMT+0800 (CST)
			2 secs (0 hrs) behind the primary 
		source: 192.168.0.122:27017
			syncedTo: Wed Nov 27 2019 13:16:45 GMT+0800 (CST)
			2 secs (0 hrs) behind the primary 
	
		repl_set:SECONDARY> db.printSlaveReplicationInfo()
		source: 192.168.0.121:27017
			syncedTo: Wed Nov 27 2019 13:16:09 GMT+0800 (CST)
			-1 secs (0 hrs) behind the primary 
		source: 192.168.0.122:27017
			syncedTo: Wed Nov 27 2019 13:16:08 GMT+0800 (CST)
			0 secs (0 hrs) behind the primary 
	
	
	
	db.test.find({}).sort({_id:-1}.limit(1)

	
	
